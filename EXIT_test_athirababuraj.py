# -*- coding: utf-8 -*-
"""EXIT TEST_AthiraBaburaj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fwDGjfeLAnZftK9TJqnwG3Z3VM9SJ8ah
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("/content/train.csv")

"""EDA"""

data.head()

data.shape

data.info()

data.dtypes

data.describe()

data.nunique()

"""Univariate Analysis"""

plt.figure(figsize=(12, 6))
sns.countplot(x='goalkeeping_skills', data=data, order=data['goalkeeping_skills'].value_counts().index)
plt.title('Counts of goalkeeping_skills')
plt.xlabel('goalkeeping_skills')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(x='shot_accuracy', data=data, order=data['shot_accuracy'].value_counts().index)
plt.title('Counts of shot_accuracy')
plt.xlabel('shot_accuracy')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(x='matches_played', data=data)

plt.figure(figsize=(30, 6))
sns.countplot(x='passing_skills', data=data)

coaching_counts = data['coaching'].value_counts()
plt.pie(coaching_counts.values, labels=coaching_counts.index, autopct='%1.1f%%')
plt.title('Distribution of coaching Status')
plt.show()

dedication_level_counts = data['dedication_level'].value_counts()
plt.pie(dedication_level_counts.values, labels=dedication_level_counts.index, autopct='%1.1f%%')
plt.title('Distribution of dedication_level Status')
plt.show()

"""Bivariate Analysis"""

plt.figure(figsize=(8, 6))
sns.boxplot(x='strong_foot', y='fitness_rating', data=data)
plt.title('strong_foot vs fitness_rating')
plt.show()

plt.figure(figsize=(25, 15))
sns.countplot(data=data, x='trophies_won', hue='dedication_level')
plt.title('Bivariate Analysis of trophies_won and dedication_level')
plt.xlabel('trophies_won')
plt.ylabel('Count')
plt.xticks(rotation=90)

# Display the plot
plt.show()

plt.figure(figsize=(25, 15))
sns.countplot(data=data, x='selection', hue='coaching')
plt.title('Bivariate Analysis of selection and coaching')
plt.xlabel('selection')
plt.ylabel('Count')
plt.xticks(rotation=90)

plt.show()

plt.figure(figsize=(25, 6))
sns.countplot(data=data, x='shot_accuracy', hue='selection')
plt.title('Bivariate Analysis of shot_accuracy and selection')
plt.xlabel('shot_accuracy')
plt.ylabel('Count')
plt.xticks(rotation=90)

# Display the plot
plt.show()

"""PreProcessing"""

data.isna().sum()

data=data.drop('player_id',axis=1)
data=data.drop('name',axis=1)

"""Player id and name are unique columns. so dropping those"""

def quotes_removal(height_str):
    return height_str.replace("'", ".")

data['height'] = data['height'].apply(quotes_removal).astype(float)

data['weight']  = data['weight'].fillna(data['weight'].mode().iloc[0])
data['weight'] = data['weight'].str.replace("lbs", '').astype(int)

data.columns

numerical_columns= [ 'age', 'height', 'weight',
       'ball_controlling_skills', 'body_reflexes', 'body_balance',
       'jumping_skills', 'penalties_conversion_rate', 'mental_strength',
       'goalkeeping_skills', 'defending_skills', 'passing_skills',
       'dribbling_skills', 'shot_accuracy', 'body_strength_stamina',
       'max_running_speed', 'behaviour_rating',
       'matches_played', 'fitness_rating', 'trophies_won', 'years_of_experience', 'no_of_disqualifications']

for column in numerical_columns:
    plt.hist(data[column], bins=10)
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

def fill_with_median(column):
    median_value = column.median()
    return column.fillna(median_value)

for column in numerical_columns:
    data[column] = fill_with_median(data[column])

categorical_columns = ['gender', 'country','strong_foot', 'dedication_level',
       'coaching']

for column in categorical_columns:
    mode_value = data[column].mode()[0]
    data[column] = data[column].fillna(mode_value)

data.isna().sum()

"""Outliers"""

for column in numerical_columns:
    plt.figure(figsize=(8, 6))
    plt.boxplot(data[column])
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.ylabel('Value')
    plt.grid(True)
    plt.show()

outliers_indices = {}
cleaned_data = data.copy()

for column in numerical_columns:
    q1 = np.percentile(cleaned_data[column], 25)
    q3 = np.percentile(cleaned_data[column], 75)
    iqr = q3 - q1
    low_limit = q1 - 1.5 * iqr
    upr_limit = q3 + 1.5 * iqr

    column_outliers_indices = cleaned_data[(cleaned_data[column] > upr_limit) | (cleaned_data[column] < low_limit)].index
    outliers_indices[column] = column_outliers_indices
    cleaned_data = cleaned_data.drop(index=column_outliers_indices)

cleaned_data

data.dtypes

"""Encoding"""

categorical_columns_onehot = ['gender','strong_foot', 'dedication_level',
       'coaching']

data = pd.get_dummies(data, columns=categorical_columns_onehot, drop_first=True)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['country'] = le.fit_transform(data['country'])

data.dtypes

"""Correlation"""

plt.figure(figsize=(40,40))
corr_matrix=data.corr()
sns.heatmap(corr_matrix,vmin=-0.2,vmax=0.9,annot=True,cmap='YlGnBu')
plt.show

data

"""Scaling"""

newList = ['weight']
for columns in newList:
  x1 = data[columns].values.reshape(-1, 1)  # Reshape to a 2D array
  from sklearn.preprocessing import MinMaxScaler
  min_max = MinMaxScaler(feature_range=(0, 1))
  x1_scaled = min_max.fit_transform(x1)

  # Convert the scaled values back to a DataFrame
  data[columns] = x1_scaled

data

"""Train test splitting of train dataset"""

X=data.drop('selection',axis=1)
y=data['selection']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

"""Model Selection"""

data['selection'].value_counts()

## Decision Tree

from sklearn.tree import DecisionTreeClassifier
dt_clf=DecisionTreeClassifier()
dt_clf.fit(X_train,y_train)

y_pred=dt_clf.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix,precision_score,recall_score,f1_score
print('Accuracy = ',accuracy_score(y_test,y_pred))
print('Precision = ',precision_score(y_test,y_pred))
print('Recall = ',recall_score(y_test,y_pred))
print('F1 Score = ',f1_score(y_test,y_pred))

# SVM

from sklearn.svm import SVC
svmclf=SVC(kernel='linear')
svmclf.fit(X_train,y_train)

y_pred=svmclf.predict(X_test)

print('Accuracy = ',accuracy_score(y_test,y_pred))
print('Precision = ',precision_score(y_test,y_pred))
print('Recall = ',recall_score(y_test,y_pred))
print('F1 Score = ',f1_score(y_test,y_pred))

# Random Forest

from sklearn.ensemble import RandomForestClassifier
rf_clf=RandomForestClassifier()
rf_clf.fit(X_train,y_train)

y_pred=rf_clf.predict(X_test)

print('Accuracy = ',accuracy_score(y_test,y_pred))
print('Precision = ',precision_score(y_test,y_pred))
print('Recall = ',recall_score(y_test,y_pred))
print('F1 Score = ',f1_score(y_test,y_pred))

#GBM

from sklearn.ensemble import GradientBoostingClassifier


# Initialize the Gradient Boosting classifier
gbm_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Train the classifier
gbm_clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = gbm_clf.predict(X_test)

print('Accuracy = ',accuracy_score(y_test,y_pred))
print('Precision = ',precision_score(y_test,y_pred))
print('Recall = ',recall_score(y_test,y_pred))
print('F1 Score = ',f1_score(y_test,y_pred))

"""Based on metrics GBM is the best model

Cross Validation
"""

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
skfold_validator = StratifiedKFold(n_splits = 3)

for train_index,test_index in skfold_validator.split(X,y):
    print('Training Index :',train_index)
    print('Test Index :',test_index)

cv_result = cross_val_score(gbm_clf,X,y,cv=skfold_validator)

cv_result

np.mean(cv_result)

"""Hyper parameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.ensemble import GradientBoostingClassifier
import numpy as np

gbm_params = {
    'learning_rate': [0.01, 0.1, 0.2, 0.3],
    'n_estimators': np.arange(50, 200, 10),
    'subsample': [0.8, 0.9, 1.0],
    'max_depth': [3, 4, 5, 6],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [None, 'sqrt', 'log2'],
    'random_state': [None, 42],
}

gbm_model = GradientBoostingClassifier()

cv_model_gbm = RandomizedSearchCV(estimator=gbm_model, param_distributions=gbm_params,
                                  scoring='f1', n_jobs=-1, return_train_score=True,
                                  cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))

cv_model_gbm.fit(X_train, y_train)

best_params_gbm = cv_model_gbm.best_params_

# Print the best parameters
print("Best Parameters for GBM (F1 Score):", best_params_gbm)

from sklearn.metrics import make_scorer, precision_score, f1_score, accuracy_score
from sklearn.ensemble import GradientBoostingClassifier


stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

gb_params = {'subsample': 0.9,
             'random_state': None,
             'n_estimators': 120,
             'min_samples_split': 2,
             'min_samples_leaf': 2,
             'max_features': None,
             'max_depth': 4,
             'learning_rate': 0.3}

precision_scores_gb = []
f1_scores_gb = []
accuracy_scores_gb = []
for train_index, test_index in stratified_kfold.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    gbm_clf = GradientBoostingClassifier(**gb_params)
    gbm_clf.fit(X_train, y_train)
    y_pred = gbm_clf.predict(X_test)

    # Calculate precision and F1 scores
    precision = precision_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    precision_scores_gb.append(precision)
    f1_scores_gb.append(f1)
    accuracy_scores_gb.append(accuracy)

print("Cross-Validation Precision Scores (Gradient Boosting):", precision_scores_gb)
print("Mean Precision (Gradient Boosting):", sum(precision_scores_gb) / len(precision_scores_gb))
print("Standard Deviation (Precision) (Gradient Boosting):", np.std(precision_scores_gb))

print("Cross-Validation F1 Scores (Gradient Boosting):", f1_scores_gb)
print("Mean F1 Score (Gradient Boosting):", sum(f1_scores_gb) / len(f1_scores_gb))
print("Standard Deviation (F1) (Gradient Boosting):", np.std(f1_scores_gb))

print("Cross-Validation accuracy Scores (Gradient Boosting):", accuracy_scores_gb)
print("Mean accuracy Score (Gradient Boosting):", sum(accuracy_scores_gb) / len(accuracy_scores_gb))
print("Standard Deviation (accuracy) (Gradient Boosting):", np.std(accuracy_scores_gb))

